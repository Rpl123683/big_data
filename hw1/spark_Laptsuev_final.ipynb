{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"JAVA_HOME\"] = '/usr/lib/jvm/java-8-openjdk-amd64'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = 'pyspark-shell'\n",
    "\n",
    "os.environ[\"SPARK_HOME\"] = '/home/act10nz/contest/spark-2.2.0-bin-hadoop2.7/'\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.environ['SPARK_HOME']+\"/python\")\n",
    "\n",
    "sys.path.append(os.environ['SPARK_HOME']+\"/python/lib/py4j-0.10.4-src.zip\")\n",
    "\n",
    "import py4j\n",
    "from pyspark import SparkContext, SparkConf, SQLContext\n",
    "\n",
    "conf = (SparkConf().setMaster(\"local[8]\")\n",
    "        .setAppName(\"ML demo\")\n",
    "        .set(\"spark.executor.memory\", \"2g\"))\n",
    "\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "sqlcontext = SQLContext(sc)\n",
    "\n",
    "from pyspark.mllib.regression import LabeledPoint, LinearRegressionModel, LinearRegressionWithSGD\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = sqlcontext.read.format(\n",
    "    'com.databricks.spark.csv').options(\n",
    "    header='true').load('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Embarked=u'Q'), Row(Embarked=u'C'), Row(Embarked=u'S'), Row(Embarked=u'')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "\n",
    "from pyspark.sql import types\n",
    "\n",
    "\n",
    "\n",
    "def Embarked_transform(x):\n",
    "    if x != None:\n",
    "        return x\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "my_udf = udf(Embarked_transform, types.StringType())\n",
    "df = df.withColumn('Embarked', my_udf(df['Embarked']))\n",
    "df.select('Embarked').distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "\n",
    "stringIndexer = StringIndexer(inputCol=\"Embarked\", outputCol=\"EmbarkedIndex\")\n",
    "model = stringIndexer.fit(df)\n",
    "indexed = model.transform(df)\n",
    "encoder = OneHotEncoder(inputCol=\"EmbarkedIndex\", outputCol=\"EmbarkedVec\")\n",
    "df_t = encoder.transform(indexed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_age(str_age):\n",
    "    try:\n",
    "        return float(str_age)\n",
    "    except:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transf(r):\n",
    "    return LabeledPoint(\n",
    "        int(r.Survived),\n",
    "        [\n",
    "            int(r.Pclass),\n",
    "            r.Sex == 'male',\n",
    "            float(r.Fare),\n",
    "            int(r.SibSp),\n",
    "            int(r.Parch),\n",
    "            parse_age(r.Age)\n",
    "            #parse_age(r.Age)*float(r.Fare)# new feature\n",
    "        ] + list(r.EmbarkedVec.toArray())\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data = df_t.rdd.map(transf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabeledPoint(1.0, [2.0,0.0,16.0,0.0,0.0,55.0,1.0,0.0,0.0])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.collect()[15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train, test = data.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[39] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.tree import RandomForest, RandomForestModel\n",
    "rfc = RandomForest.trainClassifier(train, numClasses=2,\n",
    "                             categoricalFeaturesInfo={},\n",
    "                            numTrees=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def acc(m, test):\n",
    "    values = test.map(lambda x: x.features)\n",
    "    yhat = m.predict(values)\n",
    "    y = test.map(lambda x: x.label)\n",
    "    comp = yhat.zip(y)\n",
    "    errors = comp.map(lambda x: abs(x[0]-x[1]))\n",
    "    return 1-errors.sum()/errors.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8231292517006803"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc(rfc, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# добавить 5 новых фичей\n",
    "# 3 фичи высчитываются из имеющихся\n",
    "# хотя бы одна использует udf\n",
    "\n",
    "# попробовать 3 новых модели\n",
    "\n",
    "# f1 меру"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Фича с udf который miss или мистер"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Name=u'Mr'), Row(Name=u'Mrs'), Row(Name=u' ')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def name_transform(x):\n",
    "    if \"Mrs.\" in x:\n",
    "        return \"Mrs\"\n",
    "    elif \"Mr.\" in x:\n",
    "        return \"Mr\"\n",
    "    else:\n",
    "        return ' ' \n",
    "\n",
    "my_udf = udf(name_transform, types.StringType())\n",
    "df_t = df_t.withColumn('Name', my_udf(df['Name']))\n",
    "df_t.select('Name').distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stringIndexer = StringIndexer(inputCol=\"Name\", outputCol=\"NameIndex\")\n",
    "model = stringIndexer.fit(df_t)\n",
    "indexed = model.transform(df_t)\n",
    "\n",
    "encoder = OneHotEncoder(inputCol=\"NameIndex\", outputCol=\"NameVec\")\n",
    "df_t = encoder.transform(indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transf(r):\n",
    "    return LabeledPoint(\n",
    "        int(r.Survived),\n",
    "        [\n",
    "            int(r.Pclass),\n",
    "            r.Sex == 'male',\n",
    "            float(r.Fare),\n",
    "            int(r.SibSp),\n",
    "            int(r.Parch),\n",
    "            parse_age(r.Age),\n",
    "            parse_age(r.Age)*float(r.Fare),# new feature\n",
    "            parse_age(r.Age)**2,\n",
    "            float(r.Fare)**2\n",
    "        ] + list(r.EmbarkedVec.toArray()) + \n",
    "        list(r.NameVec.toArray())   # 2 new features \n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Треним разные модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = df_t.rdd.map(transf)\n",
    "\n",
    "train, test = data.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import SVMWithSGD #1\n",
    "from pyspark.mllib.tree import GradientBoostedTrees, GradientBoostedTreesModel #2\n",
    "from pyspark.mllib.tree import DecisionTree, DecisionTreeModel #3\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "m1=SVMWithSGD.train(train,validateData=test, intercept=True)\n",
    "m2= GradientBoostedTrees.trainClassifier(train,{},maxDepth=3)\n",
    "m3=DecisionTree.trainClassifier(train,2,{},maxDepth=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Считаем для них fMeasure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "def fm(m, test):\n",
    "    values = test.map(lambda x: x.features)\n",
    "    yhat = m.predict(values)\n",
    "    yhat=yhat.map(lambda x: float(x))\n",
    "    y = test.map(lambda x: float(x.label))\n",
    "    comp = yhat.zip(y)\n",
    "    metrics = MulticlassMetrics(comp)\n",
    "    return metrics.fMeasure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('m1_f ', 0.5691056910569106)\n",
      "('m2_f ', 0.8333333333333334)\n",
      "('m3_f ', 0.7723577235772358)\n"
     ]
    }
   ],
   "source": [
    "print(\"m1_f \", fm(m1, test))\n",
    "print(\"m2_f \", fm(m2, test))\n",
    "print(\"m3_f \", fm(m3, test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
